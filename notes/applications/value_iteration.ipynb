{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Overview](#overview) \n",
    "* [Value iteration](#ekf)\n",
    "* [Code](#code)\n",
    "* [References](#refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When policy evaluation is stopped after just one update of each state, the algorithm is called **value iteration**. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1]. This application looks at the value iteration algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback to policy iteration is that each of its iterations involves a policy evaluation. This however may\n",
    "itself be an iterative computation; thus requiring multiple sweeps through the state set [1]. Furthermore, if the evaluation is done iteratively, then convergence to $V_{\\pi}$ occurs only in the limit [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above limitations of policy iterations,  the question posed is whether we could we stop earlier? [1]. Luckily, the policy evaluation step of policy iteration can truncated without loosing the convergence gurantees of the method. Moreover, this can be done in several ways [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, when policy evaluation\n",
    "is stopped after just one update of each state, the algorithm is called **value iteration**. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{k+1}(s) = max_{\\alpha}\\sum_{s^*, r}p(s^*, r | s, \\alpha)\\left[r + \\gamma V_{k}(s^*)\\right], ~~ \\forall s \\in \\mathbb{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <figure>\n",
    "  <img src=\"../imgs/value_itr_algo.png\" alt=\"Trulli\" style=\"width:70%\">\n",
    "  <figcaption>Figure 1: Value iteration algorithm. Image from [1].</figcaption>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration is obtained simply by turning the Bellman optimality equation into an update rule [1]. It requires the maximum to be taken over all the actions. Furthermore, the algorithm terminates by checking the amount of change of the value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"code\"></a> Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.mutable.ArrayBuffer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.control.Breaks._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.math.max\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.control.Breaks._\n",
    "import scala.math.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mGrid\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Grid{\n",
    "    \n",
    "    class State(val idx: Int){\n",
    "    \n",
    "        val neigbors = new ArrayBuffer[Int]()\n",
    "        for(i <- 0 until 4){\n",
    "            neigbors += -1\n",
    "        }\n",
    "        \n",
    "        def addNeighbors(neighbors: Array[Int]): Unit = {\n",
    "            require(neighbors.length == 4)\n",
    "            \n",
    "            for(n <- 0 until neighbors.length){\n",
    "                addNeighbor(n, neighbors(n))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        def addNeighbor(idx: Int, nIdx: Int): Unit = {\n",
    "            require(idx < 4)\n",
    "            neigbors(idx) = nIdx\n",
    "        }\n",
    "        \n",
    "        def getNeighbor(idx: Int): Int = {\n",
    "            require(idx < 4)\n",
    "            return neigbors(idx)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mGrid\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Grid{\n",
    "    \n",
    "    val states = new ArrayBuffer[Grid.State]()\n",
    "    \n",
    "    \n",
    "    def nStates : Int = states.length\n",
    "    def nActions: Int = 4\n",
    "    def envDynamics(state: Grid.State, action: Int): (Double, Int, Double, Boolean) = {\n",
    "        (0.25, states(state.idx).getNeighbor(action), 1.0, false)\n",
    "    }\n",
    "    \n",
    "    def getState(idx: Int): Grid.State = {\n",
    "    \n",
    "        require(idx < nStates)\n",
    "        states(idx)  \n",
    "    }\n",
    "    \n",
    "    def create(): Unit = {\n",
    "        \n",
    "        // add a new state\n",
    "        for(s <- 0 until 9){\n",
    "            \n",
    "            states += new Grid.State(s)\n",
    "        \n",
    "            if(s == 0){\n",
    "                states(s).addNeighbors(Array(0, 1, 3, 0))\n",
    "            }\n",
    "            else if(s == 1){\n",
    "                states(s).addNeighbors(Array(1, 2, 4, 0))   \n",
    "            }\n",
    "            else if(s == 2){\n",
    "                states(s).addNeighbors(Array(2, 2, 5, 1)) \n",
    "            }\n",
    "            else if(s == 3){\n",
    "                states(s).addNeighbors(Array(0, 4, 6, 3)) \n",
    "            }\n",
    "            else if(s == 4){\n",
    "                states(s).addNeighbors(Array(1, 5, 7, 3)) \n",
    "            }\n",
    "            else if(s == 5){\n",
    "                states(s).addNeighbors(Array(2, 5, 8, 4)) \n",
    "            }\n",
    "            else if(s == 6){\n",
    "                states(s).addNeighbors(Array(3, 7, 6, 6)) \n",
    "            }\n",
    "            else if(s == 7){\n",
    "                states(s).addNeighbors(Array(4, 8, 7, 6)) \n",
    "            }\n",
    "            else if(s == 8){\n",
    "                states(s).addNeighbors(Array(5, 8, 8, 7)) \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mValueIteration\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ValueIteration(val numIterations: Int, val tolerance: Double,\n",
    "                    val gamma: Double){\n",
    "    \n",
    "    val valueF = new ArrayBuffer[Double]()\n",
    "    var residual = 1.0\n",
    "    \n",
    "    def train(grid: Grid): Unit = {\n",
    "        \n",
    "        valueF.clear()\n",
    "        \n",
    "        for(i <- 0 until grid.nStates){\n",
    "            valueF += 0.0\n",
    "        }\n",
    "        \n",
    "        breakable {\n",
    "            \n",
    "            for(itr <- Range(0, numIterations)){\n",
    "\n",
    "                println(\"> Learning iteration \" + itr)\n",
    "                println(\"> Learning residual \" + residual)\n",
    "\n",
    "                step(grid)\n",
    "\n",
    "                if(residual < tolerance) break;\n",
    "            }\n",
    "        }   \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(grid: Grid): Unit = {\n",
    "        \n",
    "        var delta: Double = 0.0\n",
    "        \n",
    "        for(sIdx <- 0 until grid.nStates){\n",
    "            \n",
    "            // Do a one-step lookahead to find the best action\n",
    "            val lookAheadVals = this.one_step_lookahead(grid, grid.getState(sIdx))\n",
    "            val maxActionValue = lookAheadVals.max\n",
    "            delta = max(delta, (maxActionValue - valueF(sIdx).abs))\n",
    "            \n",
    "            // # Update the value function. Ref: Sutton book eq. 4.10.\n",
    "            valueF(sIdx) = maxActionValue\n",
    "        }\n",
    "                        \n",
    "        this.residual = delta\n",
    "        \n",
    "    }\n",
    "    \n",
    "    // Helper function to calculate the value for \n",
    "    // all action in a given state.\n",
    "    // Returns a vector of length grid.nActions containing \n",
    "    // the expected value of each action.\n",
    "    def one_step_lookahead(grid: Grid, state: Grid.State): ArrayBuffer[Double] = {\n",
    "        \n",
    "         val values = new ArrayBuffer[Double](grid.nActions)\n",
    "        \n",
    "         for(i <- 0 until grid.nActions){\n",
    "                values += 0.0\n",
    "         }\n",
    "        \n",
    "         for(i <- 0 until grid.nActions){\n",
    "                val (prob, next_state, reward, done) = grid.envDynamics(state, i)\n",
    "                val oldVal = values(i)\n",
    "                values(i) = oldVal + prob * (reward + this.gamma * valueF(next_state))\n",
    "                \n",
    "         }\n",
    "                                     \n",
    "         values  \n",
    "    }\n",
    "    \n",
    "       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgrid\u001b[39m: \u001b[32mGrid\u001b[39m = ammonite.$sess.cmd2$Helper$Grid@794b701a"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grid = new Grid\n",
    "grid.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalueFunction\u001b[39m: \u001b[32mValueIteration\u001b[39m = ammonite.$sess.cmd29$Helper$ValueIteration@44c65bd3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val valueFunction = new ValueIteration(100, 1.0e-4, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Learning iteration 0\n",
      "> Learning residual 1.0\n",
      "> Learning iteration 1\n",
      "> Learning residual 0.3330078125\n",
      "> Learning iteration 2\n",
      "> Learning residual 0.078125\n",
      "> Learning iteration 3\n",
      "> Learning residual 0.0048828125\n",
      "> Learning iteration 4\n",
      "> Learning residual 3.0517578125E-4\n"
     ]
    }
   ],
   "source": [
    "valueFunction.train(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Richard S. Sutton and Andrew G. Barto, ```Reinforcement Learning: An Introduction```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
